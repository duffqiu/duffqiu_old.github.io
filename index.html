
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Duff Qiu's Blog</title>
  <meta name="author" content="Duff Qiu">


<meta name="msvalidate.01" content="359198A91024370BBF46839C4C26D15C" />


<link href='/assets/global-6690b226d693d0dcae1c080ba74b7899.css' rel='stylesheet' type='text/css' />


<script src='/assets/global-1ae286654142297648bf164d97e30542.js' type='text/javascript'></script>


  
  <meta name="description" content="原由 在一个云集群环境中，如何部署服务使其达到高可用性是运维中重要的事情。如果选用CoreOS和Docker作为基础，那么Fleet将是一个很好的服务调度工具。不过改调度工具是比较适用于低层的服务，如果想要灵活的更小粒度的调度应用服务，则需要参考Apache &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://duffqiu.github.io">
  <link href="/favicon.png" rel="icon">
<!--
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css"> -->
  <link href="/atom.xml" rel="alternate" title="Duff Qiu's Blog" type="application/atom+xml">
  <!-- <script src="/javascripts/modernizr-2.0.js"></script> -->
  <!-- <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script> -->
  <!-- <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script> -->
  <!-- <script src="/javascripts/octopress.js" type="text/javascript"></script> -->
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href='/assets/global-f853f583f48a4b2456dd0c85319205c5.css' rel='stylesheet' type='text/css' />


<script src='/assets/global-1ae286654142297648bf164d97e30542.js' type='text/javascript'></script>


<!-- <link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css"> -->
<!-- <link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css"> -->

  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-47478128-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Duff Qiu's Blog</a></h1>
  
    <h2>Duff Qiu的个人博客，只为记录一个过程，结果都是浮云☁️！</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:duffqiu.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">首页</a></li>
  <li><a href="/blog/categories/java/">Java编程</a><li>
  <li><a href="/blog/categories/scala/">Scala编程</a><li>
  <li><a href="/blog/categories/uml/">UML Tips</a><li>
  <li><a href="/blog/categories/osx/">OSX经验与Tips</a><li>
  <li><a href="/blog/categories/octopress/">Octopress经验</a><li>
  <li><a href="/blog/archives">全部文章</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/25/fleet-ha-service-trap/">Fleet部署高可用性服务的坑</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-25T17:39:16+08:00" pubdate data-updated="true">Jul 25<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/25/fleet-ha-service-trap/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="section">原由</h3>
<hr />
<p>在一个云集群环境中，如何部署服务使其达到高可用性是运维中重要的事情。如果选用CoreOS和Docker作为基础，那么Fleet将是一个很好的服务调度工具。不过改调度工具是比较适用于低层的服务，如果想要灵活的更小粒度的调度应用服务，则需要参考Apache Mesos或者是Google的Kubernetes。至于用Mesos或者是Kubernetes，我后面试完后再分享。回到Fleet，我使用它主要是因为需要集群中重要部署某些特定的服务给应用服务使用，如zookeeper。所以参见fleet的文档“<a href="https://coreos.com/fleet/docs/latest/launching-containers-fleet.html">如使用fleet何部署容器</a>”，但是对于高可用性章节，有些注意点文档倒是没有提，给使用过程造成了一定的麻烦。这个问题也在issue列表中<a href="https://github.com/coreos/fleet/issues/969">#969</a>，在本文时改问题依然没有修复。不过如何恢复错误倒是有办法的</p>

<h3 id="section-1">解决办法</h3>
<hr />

<p>问题的来源是这样的，如果需要部署高可用性服务，那么systemd服务的命名方式是xxxx@.service。注意，文件一定要以<code>.service</code>最为后缀命名。然后通过fleet命令来部署它，本例子是部署3个服务实例</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">fleetctl submit xxxx@.service
</span><span class="line">fleetctl load xxxx@{1..3}.service
</span><span class="line">fleetctl start xxxx@{1..3}.service</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>但是在敲键盘的时候，输入<code>load</code>指令时讲要运行的服务的实例的个数给漏了，写成和 submit相同的形式，因为我是调出历史命令改的</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">fleetctl load xxxx@.service</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>这个时候fleet将出问题了，fleetctl会一直挂在哪里不动了。通过<code>ctl+c</code>取消后，使用正确的命令也不会去部署服务了（某台错误的机器）</p>

<p>通过查看fleet的服务日志<code>journalctl -u fleet</code>可以看到其一直在输出错误：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
</pre></td><td class="code"><pre><code class=""><span class="line">ul 25 09:21:59 core-02 fleetd[550]: ERROR reconcile.go:79: Unable to determine agent's current state: failed fetching unit states from UnitManager: Unit name xxxxr@.service is not valid.
</span><span class="line">Jul 25 09:21:59 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name xxxx@.service is not valid.
</span><span class="line">Jul 25 09:22:00 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name xxxx@.service is not valid.
</span><span class="line">Jul 25 09:22:01 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name xxxx@.service is not valid.
</span><span class="line">Jul 25 09:22:02 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name zookeeper@.service is not valid.
</span><span class="line">Jul 25 09:22:03 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name xxxx@.service is not valid.
</span><span class="line">Jul 25 09:22:04 core-02 fleetd[550]: ERROR reconcile.go:79: Unable to determine agent's current state: failed fetching unit states from UnitManager: Unit name xxxx@.service is not valid</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>解决步骤如下</p>

<ul>
  <li>找到出问题的机器。因为fleet是随机部署服务的，所以需要确定是哪台机才可以定位问题。通过<code>fleetctl list-unit-files</code>看看<code>xxxx@.service</code>被load到哪台机上</li>
  <li>通过ssh登陆到错误的机器，查找xxxx@.service文件所在的地方，然后将其删除。文件主要放在两个地方：<code>/run/fleet/units/</code>和<code>/run/systemd/system</code>。需要sudo权限</li>
  <li>运行<code>systemctl daemon-reload</code>刷新systemd的服务</li>
  <li>重新启动fleet: <code>systemctl restart fleet</code></li>
  <li>需要注意的是如果重启fleet将会造成所以依赖fleet管理的服务都会被重新启动。如果服务是通过docker容器运行的，而服务描述文件中有写了<code>docker pull</code>将有可能造成服务恢复需要很长时间</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/18/user-docker-mac/">在Mac下使用docker</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-18T18:29:43+08:00" pubdate data-updated="true">Jul 18<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/18/user-docker-mac/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="section">原由</h3>
<hr />
<p>Docker是基于Linux Kernel的容器管理器，但是如果想在Mac上使用就必须安装Linux的虚拟机。当然如果是自己安装一个Linux到virtualbox或者vmare也是没有问题的。我自己就是这样用CoreOS的。但是如果只是想试用或学习docker，这样的方式有些重了，因为每次都要启动虚拟机，登录虚拟机再使用。为了解决这个docker有了<a href="http://docs.docker.com/installation/mac/">boot2docker</a>。但是boot2docker还是需要比较繁琐的安装。所以最好的方式是直接使用<a href="https://kitematic.com">kitematic</a>，kitematic还同时支持windows。但是切记不要将kitematic用在生产环境。</p>

<h3 id="section-1">解决办法</h3>
<hr />

<ul>
  <li>直接下载kitematic，然后得到一个kitematic.app</li>
  <li>将它放到application中，直接运行就好</li>
  <li>在菜单中将docker命令行安装到系统，需要管理员权限，这样就可以通过命令行使用docker了(不过命令行启动需要在kitematic点击<code>DOCKER CLI</code>)</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/13/zookeeper-cloud-deploy/">Zookeeper云部署方案设计</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-13T16:50:40+08:00" pubdate data-updated="true">Jul 13<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/13/zookeeper-cloud-deploy/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="section">原由</h3>
<hr />
<p><a href="http://zookeeper.apache.org/">Zookeeper</a>从Hadoop开始就被Apache多个项目使用。其作为分布式的Key-Value的配置存储，master-election，分布式锁以及service discovery等功能被广泛使用。尽管后很多后起之秀如：<a href="https://coreos.com/etcd/">Etcd</a>, <a href="https://www.consul.io/">Consul</a>，但是因为不同框架间的依赖关系（如Mesos, Storm, Kafka等都依赖zookeeper），Zookeeper仍然是无可代替的。当然也有一种趋势是，新的一些框架／应用开始同时支持多种类似于zookeeper的框架。在选择上用哪种框架来做为配置存储等，各有各的忧虑了。而作为一个互联网的后台平台，很可能需要同时用到多种类似于zookeeper的框架。而对于如何将zookeeper部署到云上，支持scale-in, scale-out, fault tolerance, high avaiabilty等特性则很少有文章提起。本文则是通过在CoreOS上，利用docker以及CoreOS的etcd, fleet等设计如何将zookeeper部署到云平台上，并达到上面提到各种特性。</p>

<h3 id="section-1">方案设计</h3>
<hr />

<h4 id="section-2">总体设计思路</h4>

<p><img src="/images/zookeepercloud.jpg" alt="zookeeper cloud deploy" /></p>

<h4 id="section-3">设计前提</h4>

<ul>
  <li>设计上将zookeeper部署到CoreOS的主机上，并通过Docker Container的方式运行。当然这个不是必须的。也可以直接部署到linux/windows的主机上，只是管理和运维的方式略有不同，这个例子可以作为参考</li>
  <li>因为最新的zookeeper3.4.6有个bug，如果设置了zoo.cfg中使用域名的方式来作为集群中的主机名，则当如果域名对应的ip被改变后，zookeeper将无法识别。 参见<a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1506">issue 1506</a>，所以我下载了源码自己编译了一个版本</li>
  <li>据说zookeeper在后续的版本会加入自身的service discovery功能，则云化部署需要重新调整</li>
  <li>
    <p>一个zookeeper的quorum中主机数n的容错率为ceil(n/2)，即如果机器为3台则必须有2台存活；如果为5台则必须有3台存活。</p>
  </li>
  <li>zookeeper的集群是在配置的时候通过zoo.cfg的配置文件中的主机列表决定一个quorum，如以下配置。利用CoreOS的fleet做到在某个主机意外down掉后，fleet会在另外一个可用的主机上重新启动服务，所以这里不能使用固定ip，而是使用了域名。</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">server.1=zookeeper-1:2888:3888
</span><span class="line">server.2=zookeeper-2:2888:3888
</span><span class="line">server.3=zookeeper-3:2888:3888</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>因为要使用到域名，不得不使用自己的域名服务器，我使用了skydns2作为我的域名解析，它是利用etcd作为配置数据存储。（因为用了CoreOS，etcd自然就已经在了，不过在etcd切换到2.0版本的时候发生了写问题，不过那是后话了）。具体可以参见我的github上如何打包配置skeydns: <a href="https://github.com/duffqiu/skydns2">duffqiu/skydns</a></li>
  <li>
    <p>因为在zookeeper集群配置中需要特定指定集群的数量和明确其ip或域名，这将带来以下问题</p>

    <ul>
      <li>如何能动态的扩展zookeeper来支持更多的客户端，同时又不用去重启哪些已经运行的zookeeper，避免因为zookeeper重启而造成的应用的重联</li>
      <li>如何避免因为扩充了zookeeper的主机数量而造成zookeeper自身的master的选举的效率的问题</li>
      <li>如何到达在不需要的时候可以动态的减少zookeeper的主机数而不造成影响</li>
    </ul>
  </li>
  <li>根据以上问题分析，我们需要用到zookeeper的observer特性  </li>
</ul>

<h4 id="section-4">思路要点</h4>

<ul>
  <li>在一个数据中心，配置可靠的zookeeper集群核心，主机数量为3/5/7个，具体看使用情况定。有文章说不要将核心的群集中的主机分布到不同的数据中心，因为多数据中心的网络延迟和不可靠性将极大影响zookeeper集群的可用性。如本图中的core1, core2, core3。这和核心的主机配置都是通过域名的方式，以fleet的服务方式部署，当一个主机意外down后，也能通过fleet来恢复，从而达到高可用性。因为是固定的配置，这个核心是不会scale-in, scale-out的。具体可以参考<a href="https://github.com/duffqiu/zk-fleet">duffqiu/zk-fleet</a></li>
  <li>利用zookeeper的observer特性，来作为应用的接入的边缘节点，该类observer主机不参与zookeeper的master选举，而不会造成选举性能问题。因为核心的zookeeper集群可以不用配置这些边缘节点，所以这些边缘节点的scale-out, scale-in不会影响到核心集群。边缘节点的配置可以参考<a href="https://github.com/duffqiu/zk-observer">duffqiu/zk-observer</a></li>
  <li>为了屏蔽zookeeper的伸缩性对于应用的影响，则对于一组边缘节点通用使用一个反向代理作为起接入点，如zk-1会接入到zk-observer-1-1…zk-observer-1-n。这个配置我还没做，后续补上</li>
</ul>

<h4 id="tips">Tips</h4>

<p>zookeeper的默认配置对于日志文件没有限定，这样会造成磁盘的无尽消耗，需要将配置增加如下</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">autopurge.snapRetainCount=10
</span><span class="line">autopurge.purgeInterval=1</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/09/install-zsh-coreos/">Install ZSH and Oh_my_zsh in CoreOS</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-09T21:13:24+08:00" pubdate data-updated="true">Jul 9<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/09/install-zsh-coreos/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="why">Why</h3>
<hr />

<p>Since some guys from other country ask me how to install zsh and oh-my-zsh in CoreOS, I have to write it in English.</p>

<p>ZSH with <a href="https://github.com/robbyrussell/oh-my-zsh">oh_my_zsh</a> is very convinient shell instead of bash or other shells. </p>

<p>But if you are using CoreOS (most people use it because of using Docker, fleet and etcd), you will meet a challenge because CoreOS doesn’t include zsh with it.</p>

<p>Since there is no installation tool like yum or apt-get in CoreOS, we have to do some workaround mannually to have zsh on it.</p>

<h3 id="how">How</h3>
<hr />

<ul>
  <li>
    <p>Get the zsh related files. (Of course you can build a zsh from source code, but the process is too heavy)</p>

    <ul>
      <li>Create a local folder in CoreOS. For example: <code>/home/core/zsh</code>. I will refer it as <code>zsh_home</code>. And also create subfolders: <code>lib64</code>, <code>share</code>, <code>bin</code></li>
      <li>Run a docker container with centos7 image: <code>docker run -it --rm -v &lt;zsh_home&gt;:/root/zsh centos /bin/bash</code></li>
      <li>In the container, run <code>yum update</code> and <code>yum install zsh</code></li>
      <li>In the container, query what is installed for zsh: <code>rpm -aql zsh</code></li>
      <li>Copy the zsh files to the folder you mount for the container in the docker run command. <code>cp /bin/zsh root/zsh/bin</code>, <code>cp -r /usr/lib64/zsh /root/zsh/lib64</code>, <code>cp -r /usr/share/zsh /root/zsh/share</code></li>
      <li>Copy the dynamic library file for zsh: <code>cp /usr/lib64/libtinfo.so.5 /root/zsh/lib64/</code></li>
    </ul>
  </li>
  <li>Install oh-my-zsh. Follow the instruction of installation of oh-my-zsh is OK.</li>
  <li>
    <p>Update .bashrc file (under home path)</p>

    <ul>
      <li>Break the link of .bashrc: <code>rm .bashrc</code> because it is a link default</li>
      <li>Get the default version: <code>cp ../../usr/share/skel/.bashrc .bashrc</code></li>
      <li>Update .bashrc: add below lines in the bottom of the file <code>export PATH=$PATH:/home/core/zsh/bin/</code> and <code>export LD_LIBRARY_PATH=/home/core/zsh/lib64/</code></li>
      <li>add the <code>zsh</code> command in the last line of .bashrc file becuase we can’t use <code>chsh -s &lt;zsh_home&gt;/zsh</code>
. The file <code>/etc/shells</code> is read only</li>
    </ul>
  </li>
  <li>
    <p>Update oh-my-zsh</p>

    <ul>
      <li>add the below lines on the top of the file: .zshrc</li>
    </ul>
  </li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">module_path=(&lt;zsh_home&gt;/lib64/zsh/5.0.2/)
</span><span class="line">
</span><span class="line">fpath=(&lt;zsh_home&gt;/share/zsh/5.0.2/functions/ &lt;zsh_home&gt;/share/zsh/site-functions/ $fpath)
</span><span class="line">
</span><span class="line">export PATH=$PATH:&lt;zsh_home&gt;/bin</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>…
    <ul>
      <li>add <code>PATH=$PATH:/home/core/zsh/bin</code> on the top of file .oh-my-zsh/oh-my-zsh.sh</li>
      <li>add below lines on the top of the file .oh-my-zsh/tools/check_for_upgrade.sh</li>
    </ul>
  </li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
</pre></td><td class="code"><pre><code class=""><span class="line">module_path=(&lt;zsh_home&gt;/lib64/zsh/5.0.2/)
</span><span class="line">
</span><span class="line">fpath=(&lt;zsh_home&gt;/share/zsh/5.0.2/functions/ &lt;zsh_home&gt;/share/zsh/site-functions/ $fpath)</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>relogin CoreOS shall be ok to use zsh</li>
</ul>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/06/docker-cleanup-none-images/">Docker清除none Images</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-06T15:32:40+08:00" pubdate data-updated="true">Jul 6<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/06/docker-cleanup-none-images/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="section">原由</h3>
<hr />
<p>如果是本地去build image，则中间过程会产生不少<none>的image，特别是如果build的过程使用了ctrl+c取消进程的执行。如何有效的清除这些<none>
的image而节约空间呢？</none></none></p>

<h3 id="section-1">解决办法</h3>
<hr />

<p>Docker在查询image的命令中增加了<code>--filer/-f</code>参数，利用该选项查询哪些none的image然后再逐一删除</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">docker rmi $(docker images -f "dangling=true" -q)</span></code></pre></td></tr></table></div></figure></notextile></div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/05/coreos-fleet-trap/">Coreos Fleet使用陷阱几例</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-05T18:57:28+08:00" pubdate data-updated="true">Jul 5<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/05/coreos-fleet-trap/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="section">原由</h3>
<hr />
<p>CoreOS带了一个基于Systemd的集群服务管理工具，可以便于管理在多个CoreOS实例中部署和管理服务，虽然它的力度比较粗一点，无法针对资源的情况来管理，不像Mesos，但是因为其基于Systemd，其服务依赖等做的比较好，比较适用于基础服务的部署。
但是在使用过程中还是遇到了不少坑和麻烦，现在一一列举出来</p>

<h3 id="fleet">Fleet的坑</h3>
<hr />

<ul>
  <li>Fleet的Service Unit的文件的写法。在Unit文件中使用环境变量只能用在<code>[Service]</code>域中，其它域是无法使用的。参见<a href="https://github.com/coreos/fleet/issues/1246">issue 1246</a></li>
  <li>Fleet的Service Unit的文件的[X-Fleet]域的Conflicts不要像systemd那样一行写多个服务的名字。如果有多个冲突项，则需要写多行的Conflicts。参见<a href="https://github.com/coreos/fleet/issues/1245">issue 1245</a></li>
  <li>Fleetctl要能够连接集群中的其它机器，则需要使用ssh-agent</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">eval `ssh-agent`
</span><span class="line">ssh-add &lt;ssh的私钥，如果是用openstack，则是用openstack生成的私钥&gt;</span></code></pre></td></tr></table></div></figure></notextile></div>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/05/systemd-boot/">Systemd的启动顺序和自动启动配置</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-05T17:56:27+08:00" pubdate data-updated="true">Jul 5<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/05/systemd-boot/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="systemd">Systemd的启动顺序</h3>
<hr />

<ul>
  <li>/etc/systemd/system/</li>
  <li>/run/systemd/system/</li>
  <li>/usr/lib/systemd/system/</li>
</ul>

<p>在CoreOS中，自带的服务都是放在了/usr/lib/systemd/system/目录下，但是该目录是只读的，如需更改，可以将对应的service unit文件copy到/etc/systemd/system目录下，然后更改后重新启动</p>

<p>每次更改service unit文件需要执行 <code>sudo systemctl daemon-reload</code></p>

<h3 id="systemdboot">Systemd的服务需要在系统boot时启动</h3>
<hr />

<p>需要在Service Unit文件中加入：</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
</pre></td><td class="code"><pre><code class=""><span class="line">[Installl]
</span><span class="line">WantedBy=multi-user.target</span></code></pre></td></tr></table></div></figure></notextile></div>

<p>然后执行<code>sudo systemctl daemon-reload</code>和<code>sudo systemctl enable &lt;xxxx.service&gt;</code></p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/05/openstack-blockservice/">Openstack如何更好的使用block Service作为磁盘使用</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-05T17:15:59+08:00" pubdate data-updated="true">Jul 5<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/05/openstack-blockservice/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="section">原由</h3>
<hr />
<p>在使用Openstack过程中，VM总是要运行某些程序，而这些程序的数据是记录在磁盘中的。如何保证在VM被删除或者重建后这些数据依然能够存在呢？这就要用到Openstack的Block Service了。但是这里面有几个问题：</p>

<ul>
  <li>block service创建出来的卷是是个无文件格式的盘，无法被直接读写</li>
  <li>通常我们希望在给一个VM初始化时挂在一个盘时，这个盘已经包含了某些必须的文件内容 </li>
</ul>

<h3 id="section-1">解决办法</h3>
<hr />

<p>思路是做出一个带有文件格式已经需要的文件内容／数据的block service的image，这样就不用去格式化这个卷，同时也包含所需的内容，具体方法如下：</p>

<ul>
  <li>创建一个本地image，指定大小。 <code>dd if=/dev/null of=example.img bs=1M seek=1024</code>。这里创建了一个1G大小的镜像文件example.img，对于不同的使用目的，这里的bs需要调优</li>
  <li>格式化成ext4。 <code>mkfs.ext4 -F example.img</code></li>
  <li>创建本地的挂载目录 <code>mkdir &lt;path&gt;</code></li>
  <li>挂载这个image到指定的目录 <code>mount -t ext4 -o loop example.img &lt;path&gt;</code></li>
  <li>将需要的资料放入到这个目录中</li>
  <li>卸载该image <code>umount &lt;path&gt;</code></li>
  <li>将image转成qcow2格式 <code>qemu-img convert -f raw -O qcow2 example.img example.qcow2</code></li>
  <li>上传到openstack</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
</pre></td><td class="code"><pre><code class=""><span class="line">glance image-create --name &lt;image name&gt; \
</span><span class="line">--container-format bare \
</span><span class="line">--disk-format qcow2 \
</span><span class="line">--file exmaple.qcow2 \
</span><span class="line">--is-public True</span></code></pre></td></tr></table></div></figure></notextile></div>

<ul>
  <li>创建block storage的卷</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">cinder create --image-id &lt;image uuid&gt; --display-name &lt;disk name&gt; &lt;size x G&gt;</span></code></pre></td></tr></table></div></figure></notextile></div>
<ul>
  <li>创建完成就可以将其挂载到VM中使用了</li>
  <li>可以在创建VM时在nova命令中生成这个卷</li>
</ul>

<h4 id="openstack-tips">Openstack tips</h4>

<ul>
  <li><code>vgdisplay</code>获取openstack的磁盘空间信息</li>
</ul>

<h4 id="bash-tips">Bash tips</h4>

<ul>
  <li><code>echo $?</code> 获取最近命令的status code</li>
</ul>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/05/coreos-etcd2/">CoreOS Etcd2</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-05T16:13:19+08:00" pubdate data-updated="true">Jul 5<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/05/coreos-etcd2/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="section">原由</h3>
<hr />
<p>从CoreOS 682.1开始，ETCD2已经被默认安装了，通过Clould Init的方式在Openstack上启动CoreOS时发生了一些错误，总结如下</p>

<h3 id="section-1">解决办法</h3>
<hr />

<ul>
  <li>ETCD2总是启动不成功，通过<code>journalctl _EXE=/usr/bin/coreos-cloudinit</code>查看发现其提示没有发现本地的文件有相关的member信息。究其原因是我在cloudinit中设置了etcd2的文件路径，而改文件目录默认是root的权限，但是etcd2运行的时候是以etcd用户运行的，所以造成无法访问改文件目录而启动失败</li>
  <li>解决的办法是通过在cloudinit中写一个oneshot的systemd的服务，该服务将这个目录的权限和所有权更改为etcd，可参照我github上的<a href="https://raw.githubusercontent.com/duffqiu/coreos-openstack/master/etcd2/cloud-config-front1.yaml">cloudinit的例子</a></li>
  <li>另外需要注意的是，etcd2通过discovery指定的cluster的大小。如果没有满足这个cluster的大小的节点联入则etcd2的集群是无法启动的</li>
  <li>etcd2的cluster，在接入节点满足cluster指定的大小后，后续的接入节点将自动降格为proxy模式</li>
  <li>etcdctl可以用<code>--debug</code>开查看发出的CURL命令是什么</li>
</ul>

<h4 id="docker-tips">Docker tips</h4>

<ul>
  <li>从Docker 1.6开始，docker可以支持查看container的运行时的CPU和Memery的消耗，以下命令为列出本机所有containter的运行状况</li>
</ul>

<p><code>docker stats $(docker inspect --format='' $(docker ps -q))</code></p>

<ul>
  <li>从Docker 1.6开始，docker支持直接使用命令进入到container中进行交互，而不用向以前那么麻烦的使用nsenter的方式了，具体用法是</li>
</ul>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
</pre></td><td class="code"><pre><code class=""><span class="line">docker exec -it &lt;containter name&gt; &lt;command in container&gt;</span></code></pre></td></tr></table></div></figure></notextile></div>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2015/07/05/coreos-keepalived/">CoreOS安装和使用Keepalived</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2015-07-05T15:43:47+08:00" pubdate data-updated="true">Jul 5<span>th</span>, 2015</time>
        
        
           | <a href="/blog/2015/07/05/coreos-keepalived/#comments">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><h3 id="section">原由</h3>
<hr />
<p>在之前的文章<a href="/blog/2015/07/05/openstack-vip/">Openstack用Keealived来实现VIP</a>介绍了如何使用Keepalived来实现VIP，但是如果使用的vm是CoreOS，所有的程序又必须运行在Docker中的话，又该如何办呢？</p>

<h3 id="section-1">解决办法</h3>
<hr />

<ul>
  <li>将Keepalived做成一个Docker的image，如果需要可以自己下载keepalived的源码编译出可执行文件。因为只是用到VIP特性，可以在编译的时候将IPVS去掉（该功能可以使用haproxy代替）。一个比较偷懒的办法是先用yum的方式安装keepalived(基于CentOS)<code>yum install keepalived</code>，然后用<code>rpm -e --nodeps keepalived</code>去掉安装的可执行文件而是用自己编译的可执行文件。不然的话就需要一个一个指定的方式来安装keepalived需要的依赖包了</li>
  <li>查看CoreOS的内核是否已经启动ip_vs模块<code>lsmod</code>。似乎从681.2开始，默认都已经启动了，之前的版本没有默认启动</li>
  <li>如果没有启动ip_vs，则可以通过<code>sudo modprobe -a ip_vs</code>来加载</li>
  <li>使用<code>--priviliedged</code>和<code>--net=host</code>的方式来启动keepalived</li>
  <li>具体的docker file可以参考我的github上的<a href="https://github.com/duffqiu/keepalived">repo</a></li>
</ul>

</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2015/07/25/fleet-ha-service-trap/">Fleet部署高可用性服务的坑</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/07/18/user-docker-mac/">在Mac下使用docker</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/07/13/zookeeper-cloud-deploy/">Zookeeper云部署方案设计</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/07/09/install-zsh-coreos/">Install ZSH and Oh_my_zsh in CoreOS</a>
      </li>
    
      <li class="post">
        <a href="/blog/2015/07/06/docker-cleanup-none-images/">Docker清除none Images</a>
      </li>
    
  </ul>
</section>




<section>
  <h1>Categories</h1>
  <ul id="categories">
    <li class='category'><a href='/blog/categories/appscale/'>Appscale (3)</a></li>
<li class='category'><a href='/blog/categories/auto-test/'>Auto Test (1)</a></li>
<li class='category'><a href='/blog/categories/bing/'>Bing (1)</a></li>
<li class='category'><a href='/blog/categories/block-storage/'>Block Storage (1)</a></li>
<li class='category'><a href='/blog/categories/centos/'>CentOS (1)</a></li>
<li class='category'><a href='/blog/categories/cloud/'>Cloud (1)</a></li>
<li class='category'><a href='/blog/categories/coreos/'>CoreOS (6)</a></li>
<li class='category'><a href='/blog/categories/data-model/'>Data Model (1)</a></li>
<li class='category'><a href='/blog/categories/design-pattern/'>Design Pattern (9)</a></li>
<li class='category'><a href='/blog/categories/docker/'>Docker (2)</a></li>
<li class='category'><a href='/blog/categories/etcd2/'>ETCD2 (1)</a></li>
<li class='category'><a href='/blog/categories/entity-model/'>Entity Model (1)</a></li>
<li class='category'><a href='/blog/categories/fp/'>FP (1)</a></li>
<li class='category'><a href='/blog/categories/findbugs/'>Findbugs (1)</a></li>
<li class='category'><a href='/blog/categories/fleet/'>Fleet (2)</a></li>
<li class='category'><a href='/blog/categories/gsl/'>GSL (1)</a></li>
<li class='category'><a href='/blog/categories/guava/'>Guava (1)</a></li>
<li class='category'><a href='/blog/categories/guice/'>Guice (2)</a></li>
<li class='category'><a href='/blog/categories/integration/'>Integration (1)</a></li>
<li class='category'><a href='/blog/categories/junit/'>JUnit (1)</a></li>
<li class='category'><a href='/blog/categories/java/'>Java (13)</a></li>
<li class='category'><a href='/blog/categories/jekyll/'>Jekyll (1)</a></li>
<li class='category'><a href='/blog/categories/jenkins/'>Jenkins (3)</a></li>
<li class='category'><a href='/blog/categories/keepalived/'>Keepalived (2)</a></li>
<li class='category'><a href='/blog/categories/linux/'>Linux (1)</a></li>
<li class='category'><a href='/blog/categories/mac/'>MAC (3)</a></li>
<li class='category'><a href='/blog/categories/maven/'>Maven (3)</a></li>
<li class='category'><a href='/blog/categories/miscellany/'>Miscellany (1)</a></li>
<li class='category'><a href='/blog/categories/oo/'>OO (1)</a></li>
<li class='category'><a href='/blog/categories/osx/'>OSX (4)</a></li>
<li class='category'><a href='/blog/categories/octopress/'>Octopress (16)</a></li>
<li class='category'><a href='/blog/categories/openstack/'>Openstack (2)</a></li>
<li class='category'><a href='/blog/categories/plantuml/'>PlantUML (3)</a></li>
<li class='category'><a href='/blog/categories/port/'>Port (2)</a></li>
<li class='category'><a href='/blog/categories/proxy/'>Proxy (2)</a></li>
<li class='category'><a href='/blog/categories/rest/'>REST (3)</a></li>
<li class='category'><a href='/blog/categories/sbt/'>SBT (2)</a></li>
<li class='category'><a href='/blog/categories/scala/'>Scala (7)</a></li>
<li class='category'><a href='/blog/categories/script/'>Script (1)</a></li>
<li class='category'><a href='/blog/categories/sublime/'>Sublime (1)</a></li>
<li class='category'><a href='/blog/categories/systemd/'>Systemd (1)</a></li>
<li class='category'><a href='/blog/categories/uml/'>UML (1)</a></li>
<li class='category'><a href='/blog/categories/virtualbox/'>VirtualBox (1)</a></li>
<li class='category'><a href='/blog/categories/zsh/'>ZSH (1)</a></li>
<li class='category'><a href='/blog/categories/zookeeper/'>Zookeeper (1)</a></li>

  </ul>
</section>
<section>
     <h1>About Me</h1>
     <iframe width="100%" height="550" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=550&fansRow=2&ptype=1&speed=300&skin=1&isTitle=0&noborder=0&isWeibo=1&isFans=0&uid=1872168377&verifier=1cd3a528&dpc=1"></iframe>
</section>

  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2015 - Duff Qiu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  












</body>
</html>
