<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Zookeeper | Duff Qiu's Blog]]></title>
  <link href="http://duffqiu.github.io/blog/categories/zookeeper/atom.xml" rel="self"/>
  <link href="http://duffqiu.github.io/"/>
  <updated>2015-08-14T14:47:26+08:00</updated>
  <id>http://duffqiu.github.io/</id>
  <author>
    <name><![CDATA[Duff Qiu]]></name>
    <email><![CDATA[duffqiu@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Zookeeper云部署方案设计]]></title>
    <link href="http://duffqiu.github.io/blog/2015/07/13/zookeeper-cloud-deploy/"/>
    <updated>2015-07-13T16:50:40+08:00</updated>
    <id>http://duffqiu.github.io/blog/2015/07/13/zookeeper-cloud-deploy</id>
    <content type="html"><![CDATA[<h3 id="section">原由</h3>
<hr />
<p><a href="http://zookeeper.apache.org/">Zookeeper</a>从Hadoop开始就被Apache多个项目使用。其作为分布式的Key-Value的配置存储，master-election，分布式锁以及service discovery等功能被广泛使用。尽管后很多后起之秀如：<a href="https://coreos.com/etcd/">Etcd</a>, <a href="https://www.consul.io/">Consul</a>，但是因为不同框架间的依赖关系（如Mesos, Storm, Kafka等都依赖zookeeper），Zookeeper仍然是无可代替的。当然也有一种趋势是，新的一些框架／应用开始同时支持多种类似于zookeeper的框架。在选择上用哪种框架来做为配置存储等，各有各的忧虑了。而作为一个互联网的后台平台，很可能需要同时用到多种类似于zookeeper的框架。而对于如何将zookeeper部署到云上，支持scale-in, scale-out, fault tolerance, high avaiabilty等特性则很少有文章提起。本文则是通过在CoreOS上，利用docker以及CoreOS的etcd, fleet等设计如何将zookeeper部署到云平台上，并达到上面提到各种特性。</p>

<h3 id="section-1">方案设计</h3>
<hr />

<h4 id="section-2">总体设计思路</h4>

<p><img src="/images/zookeepercloud.jpg" alt="zookeeper cloud deploy" /></p>

<h4 id="section-3">设计前提</h4>

<ul>
  <li>设计上将zookeeper部署到CoreOS的主机上，并通过Docker Container的方式运行。当然这个不是必须的。也可以直接部署到linux/windows的主机上，只是管理和运维的方式略有不同，这个例子可以作为参考</li>
  <li>因为最新的zookeeper3.4.6有个bug，如果设置了zoo.cfg中使用域名的方式来作为集群中的主机名，则当如果域名对应的ip被改变后，zookeeper将无法识别。 参见<a href="https://issues.apache.org/jira/browse/ZOOKEEPER-1506">issue 1506</a>，所以我下载了源码自己编译了一个版本</li>
  <li>据说zookeeper在后续的版本会加入自身的service discovery功能，则云化部署需要重新调整</li>
  <li>
    <p>一个zookeeper的quorum中主机数n的容错率为ceil(n/2)，即如果机器为3台则必须有2台存活；如果为5台则必须有3台存活。</p>
  </li>
  <li>zookeeper的集群是在配置的时候通过zoo.cfg的配置文件中的主机列表决定一个quorum，如以下配置。利用CoreOS的fleet做到在某个主机意外down掉后，fleet会在另外一个可用的主机上重新启动服务，所以这里不能使用固定ip，而是使用了域名。</li>
</ul>

<p><code>
server.1=zookeeper-1:2888:3888
server.2=zookeeper-2:2888:3888
server.3=zookeeper-3:2888:3888
</code></p>

<ul>
  <li>因为要使用到域名，不得不使用自己的域名服务器，我使用了skydns2作为我的域名解析，它是利用etcd作为配置数据存储。（因为用了CoreOS，etcd自然就已经在了，不过在etcd切换到2.0版本的时候发生了写问题，不过那是后话了）。具体可以参见我的github上如何打包配置skeydns: <a href="https://github.com/duffqiu/skydns2">duffqiu/skydns</a></li>
  <li>
    <p>因为在zookeeper集群配置中需要特定指定集群的数量和明确其ip或域名，这将带来以下问题</p>

    <ul>
      <li>如何能动态的扩展zookeeper来支持更多的客户端，同时又不用去重启哪些已经运行的zookeeper，避免因为zookeeper重启而造成的应用的重联</li>
      <li>如何避免因为扩充了zookeeper的主机数量而造成zookeeper自身的master的选举的效率的问题</li>
      <li>如何到达在不需要的时候可以动态的减少zookeeper的主机数而不造成影响</li>
    </ul>
  </li>
  <li>根据以上问题分析，我们需要用到zookeeper的observer特性  </li>
</ul>

<h4 id="section-4">思路要点</h4>

<ul>
  <li>在一个数据中心，配置可靠的zookeeper集群核心，主机数量为3/5/7个，具体看使用情况定。有文章说不要将核心的群集中的主机分布到不同的数据中心，因为多数据中心的网络延迟和不可靠性将极大影响zookeeper集群的可用性。如本图中的core1, core2, core3。这和核心的主机配置都是通过域名的方式，以fleet的服务方式部署，当一个主机意外down后，也能通过fleet来恢复，从而达到高可用性。因为是固定的配置，这个核心是不会scale-in, scale-out的。具体可以参考<a href="https://github.com/duffqiu/zk-fleet">duffqiu/zk-fleet</a></li>
  <li>利用zookeeper的observer特性，来作为应用的接入的边缘节点，该类observer主机不参与zookeeper的master选举，而不会造成选举性能问题。因为核心的zookeeper集群可以不用配置这些边缘节点，所以这些边缘节点的scale-out, scale-in不会影响到核心集群。边缘节点的配置可以参考<a href="https://github.com/duffqiu/zk-observer">duffqiu/zk-observer</a></li>
  <li>为了屏蔽zookeeper的伸缩性对于应用的影响，则对于一组边缘节点通用使用一个反向代理作为起接入点，如zk-1会接入到zk-observer-1-1…zk-observer-1-n。这个配置我还没做，后续补上</li>
</ul>

<h4 id="tips">Tips</h4>

<p>zookeeper的默认配置对于日志文件没有限定，这样会造成磁盘的无尽消耗，需要将配置增加如下</p>

<p><code>
autopurge.snapRetainCount=10
autopurge.purgeInterval=1
</code></p>
]]></content>
  </entry>
  
</feed>
