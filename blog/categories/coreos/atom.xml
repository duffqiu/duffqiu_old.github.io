<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: CoreOS | Duff Qiu's Blog]]></title>
  <link href="http://duffqiu.github.io/blog/categories/coreos/atom.xml" rel="self"/>
  <link href="http://duffqiu.github.io/"/>
  <updated>2015-08-14T14:39:56+08:00</updated>
  <id>http://duffqiu.github.io/</id>
  <author>
    <name><![CDATA[Duff Qiu]]></name>
    <email><![CDATA[duffqiu@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Fleet部署高可用性服务的坑]]></title>
    <link href="http://duffqiu.github.io/blog/2015/07/25/fleet-ha-service-trap/"/>
    <updated>2015-07-25T17:39:16+08:00</updated>
    <id>http://duffqiu.github.io/blog/2015/07/25/fleet-ha-service-trap</id>
    <content type="html"><![CDATA[<h3 id="section">原由</h3>
<hr />
<p>在一个云集群环境中，如何部署服务使其达到高可用性是运维中重要的事情。如果选用CoreOS和Docker作为基础，那么Fleet将是一个很好的服务调度工具。不过改调度工具是比较适用于低层的服务，如果想要灵活的更小粒度的调度应用服务，则需要参考Apache Mesos或者是Google的Kubernetes。至于用Mesos或者是Kubernetes，我后面试完后再分享。回到Fleet，我使用它主要是因为需要集群中重要部署某些特定的服务给应用服务使用，如zookeeper。所以参见fleet的文档“<a href="https://coreos.com/fleet/docs/latest/launching-containers-fleet.html">如使用fleet何部署容器</a>”，但是对于高可用性章节，有些注意点文档倒是没有提，给使用过程造成了一定的麻烦。这个问题也在issue列表中<a href="https://github.com/coreos/fleet/issues/969">#969</a>，在本文时改问题依然没有修复。不过如何恢复错误倒是有办法的</p>

<h3 id="section-1">解决办法</h3>
<hr />

<p>问题的来源是这样的，如果需要部署高可用性服务，那么systemd服务的命名方式是xxxx@.service。注意，文件一定要以<code>.service</code>最为后缀命名。然后通过fleet命令来部署它，本例子是部署3个服务实例</p>

<p><code>
fleetctl submit xxxx@.service
fleetctl load xxxx@{1..3}.service
fleetctl start xxxx@{1..3}.service
</code></p>

<p>但是在敲键盘的时候，输入<code>load</code>指令时讲要运行的服务的实例的个数给漏了，写成和 submit相同的形式，因为我是调出历史命令改的</p>

<p><code>
fleetctl load xxxx@.service
</code></p>

<p>这个时候fleet将出问题了，fleetctl会一直挂在哪里不动了。通过<code>ctl+c</code>取消后，使用正确的命令也不会去部署服务了（某台错误的机器）</p>

<p>通过查看fleet的服务日志<code>journalctl -u fleet</code>可以看到其一直在输出错误：</p>

<p><code>
ul 25 09:21:59 core-02 fleetd[550]: ERROR reconcile.go:79: Unable to determine agent's current state: failed fetching unit states from UnitManager: Unit name xxxxr@.service is not valid.
Jul 25 09:21:59 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name xxxx@.service is not valid.
Jul 25 09:22:00 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name xxxx@.service is not valid.
Jul 25 09:22:01 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name xxxx@.service is not valid.
Jul 25 09:22:02 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name zookeeper@.service is not valid.
Jul 25 09:22:03 core-02 fleetd[550]: ERROR generator.go:65: Failed fetching current unit states: Unit name xxxx@.service is not valid.
Jul 25 09:22:04 core-02 fleetd[550]: ERROR reconcile.go:79: Unable to determine agent's current state: failed fetching unit states from UnitManager: Unit name xxxx@.service is not valid
</code></p>

<p>解决步骤如下</p>

<ul>
  <li>找到出问题的机器。因为fleet是随机部署服务的，所以需要确定是哪台机才可以定位问题。通过<code>fleetctl list-unit-files</code>看看<code>xxxx@.service</code>被load到哪台机上</li>
  <li>通过ssh登陆到错误的机器，查找xxxx@.service文件所在的地方，然后将其删除。文件主要放在两个地方：<code>/run/fleet/units/</code>和<code>/run/systemd/system</code>。需要sudo权限</li>
  <li>运行<code>systemctl daemon-reload</code>刷新systemd的服务</li>
  <li>重新启动fleet: <code>systemctl restart fleet</code></li>
  <li>需要注意的是如果重启fleet将会造成所以依赖fleet管理的服务都会被重新启动。如果服务是通过docker容器运行的，而服务描述文件中有写了<code>docker pull</code>将有可能造成服务恢复需要很长时间</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Install ZSH and Oh_my_zsh in CoreOS]]></title>
    <link href="http://duffqiu.github.io/blog/2015/07/09/install-zsh-coreos/"/>
    <updated>2015-07-09T21:13:24+08:00</updated>
    <id>http://duffqiu.github.io/blog/2015/07/09/install-zsh-coreos</id>
    <content type="html"><![CDATA[<h3 id="why">Why</h3>
<hr />

<p>Since some guys from other country ask me how to install zsh and oh-my-zsh in CoreOS, I have to write it in English.</p>

<p>ZSH with <a href="https://github.com/robbyrussell/oh-my-zsh">oh_my_zsh</a> is very convinient shell instead of bash or other shells. </p>

<p>But if you are using CoreOS (most people use it because of using Docker, fleet and etcd), you will meet a challenge because CoreOS doesn’t include zsh with it.</p>

<p>Since there is no installation tool like yum or apt-get in CoreOS, we have to do some workaround mannually to have zsh on it.</p>

<h3 id="how">How</h3>
<hr />

<ul>
  <li>
    <p>Get the zsh related files. (Of course you can build a zsh from source code, but the process is too heavy)</p>

    <ul>
      <li>Create a local folder in CoreOS. For example: <code>/home/core/zsh</code>. I will refer it as <code>zsh_home</code>. And also create subfolders: <code>lib64</code>, <code>share</code>, <code>bin</code></li>
      <li>Run a docker container with centos7 image: <code>docker run -it --rm -v &lt;zsh_home&gt;:/root/zsh centos /bin/bash</code></li>
      <li>In the container, run <code>yum update</code> and <code>yum install zsh</code></li>
      <li>In the container, query what is installed for zsh: <code>rpm -aql zsh</code></li>
      <li>Copy the zsh files to the folder you mount for the container in the docker run command. <code>cp /bin/zsh root/zsh/bin</code>, <code>cp -r /usr/lib64/zsh /root/zsh/lib64</code>, <code>cp -r /usr/share/zsh /root/zsh/share</code></li>
      <li>Copy the dynamic library file for zsh: <code>cp /usr/lib64/libtinfo.so.5 /root/zsh/lib64/</code></li>
    </ul>
  </li>
  <li>Install oh-my-zsh. Follow the instruction of installation of oh-my-zsh is OK.</li>
  <li>
    <p>Update .bashrc file (under home path)</p>

    <ul>
      <li>Break the link of .bashrc: <code>rm .bashrc</code> because it is a link default</li>
      <li>Get the default version: <code>cp ../../usr/share/skel/.bashrc .bashrc</code></li>
      <li>Update .bashrc: add below lines in the bottom of the file <code>export PATH=$PATH:/home/core/zsh/bin/</code> and <code>export LD_LIBRARY_PATH=/home/core/zsh/lib64/</code></li>
      <li>add the <code>zsh</code> command in the last line of .bashrc file becuase we can’t use <code>chsh -s &lt;zsh_home&gt;/zsh</code>
. The file <code>/etc/shells</code> is read only</li>
    </ul>
  </li>
  <li>
    <p>Update oh-my-zsh</p>

    <ul>
      <li>add the below lines on the top of the file: .zshrc</li>
    </ul>
  </li>
</ul>

<p>```
module_path=(<zsh_home>/lib64/zsh/5.0.2/)</zsh_home></p>

<p>fpath=(<zsh_home>/share/zsh/5.0.2/functions/ <zsh_home>/share/zsh/site-functions/ $fpath)</zsh_home></zsh_home></p>

<p>export PATH=$PATH:<zsh_home>/bin
```</zsh_home></p>

<ul>
  <li>…
    <ul>
      <li>add <code>PATH=$PATH:/home/core/zsh/bin</code> on the top of file .oh-my-zsh/oh-my-zsh.sh</li>
      <li>add below lines on the top of the file .oh-my-zsh/tools/check_for_upgrade.sh</li>
    </ul>
  </li>
</ul>

<p>```
module_path=(<zsh_home>/lib64/zsh/5.0.2/)</zsh_home></p>

<p>fpath=(<zsh_home>/share/zsh/5.0.2/functions/ <zsh_home>/share/zsh/site-functions/ $fpath)
```   </zsh_home></zsh_home></p>

<ul>
  <li>relogin CoreOS shall be ok to use zsh</li>
</ul>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Coreos Fleet使用陷阱几例]]></title>
    <link href="http://duffqiu.github.io/blog/2015/07/05/coreos-fleet-trap/"/>
    <updated>2015-07-05T18:57:28+08:00</updated>
    <id>http://duffqiu.github.io/blog/2015/07/05/coreos-fleet-trap</id>
    <content type="html"><![CDATA[<h3 id="section">原由</h3>
<hr />
<p>CoreOS带了一个基于Systemd的集群服务管理工具，可以便于管理在多个CoreOS实例中部署和管理服务，虽然它的力度比较粗一点，无法针对资源的情况来管理，不像Mesos，但是因为其基于Systemd，其服务依赖等做的比较好，比较适用于基础服务的部署。
但是在使用过程中还是遇到了不少坑和麻烦，现在一一列举出来</p>

<h3 id="fleet">Fleet的坑</h3>
<hr />

<ul>
  <li>Fleet的Service Unit的文件的写法。在Unit文件中使用环境变量只能用在<code>[Service]</code>域中，其它域是无法使用的。参见<a href="https://github.com/coreos/fleet/issues/1246">issue 1246</a></li>
  <li>Fleet的Service Unit的文件的[X-Fleet]域的Conflicts不要像systemd那样一行写多个服务的名字。如果有多个冲突项，则需要写多行的Conflicts。参见<a href="https://github.com/coreos/fleet/issues/1245">issue 1245</a></li>
  <li>Fleetctl要能够连接集群中的其它机器，则需要使用ssh-agent</li>
</ul>

<p><code>
eval `ssh-agent`
ssh-add &lt;ssh的私钥，如果是用openstack，则是用openstack生成的私钥&gt;
</code></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CoreOS Etcd2]]></title>
    <link href="http://duffqiu.github.io/blog/2015/07/05/coreos-etcd2/"/>
    <updated>2015-07-05T16:13:19+08:00</updated>
    <id>http://duffqiu.github.io/blog/2015/07/05/coreos-etcd2</id>
    <content type="html"><![CDATA[<h3 id="section">原由</h3>
<hr />
<p>从CoreOS 682.1开始，ETCD2已经被默认安装了，通过Clould Init的方式在Openstack上启动CoreOS时发生了一些错误，总结如下</p>

<h3 id="section-1">解决办法</h3>
<hr />

<ul>
  <li>ETCD2总是启动不成功，通过<code>journalctl _EXE=/usr/bin/coreos-cloudinit</code>查看发现其提示没有发现本地的文件有相关的member信息。究其原因是我在cloudinit中设置了etcd2的文件路径，而改文件目录默认是root的权限，但是etcd2运行的时候是以etcd用户运行的，所以造成无法访问改文件目录而启动失败</li>
  <li>解决的办法是通过在cloudinit中写一个oneshot的systemd的服务，该服务将这个目录的权限和所有权更改为etcd，可参照我github上的<a href="https://raw.githubusercontent.com/duffqiu/coreos-openstack/master/etcd2/cloud-config-front1.yaml">cloudinit的例子</a></li>
  <li>另外需要注意的是，etcd2通过discovery指定的cluster的大小。如果没有满足这个cluster的大小的节点联入则etcd2的集群是无法启动的</li>
  <li>etcd2的cluster，在接入节点满足cluster指定的大小后，后续的接入节点将自动降格为proxy模式</li>
  <li>etcdctl可以用<code>--debug</code>开查看发出的CURL命令是什么</li>
</ul>

<h4 id="docker-tips">Docker tips</h4>

<ul>
  <li>从Docker 1.6开始，docker可以支持查看container的运行时的CPU和Memery的消耗，以下命令为列出本机所有containter的运行状况</li>
</ul>

<p>
docker stats $(docker inspect –format=”{{ .Name }}” $(docker ps -q))
</p>

<ul>
  <li>从Docker 1.6开始，docker支持直接使用命令进入到container中进行交互，而不用向以前那么麻烦的使用nsenter的方式了，具体用法是</li>
</ul>

<p><code>sh
docker exec -it &lt;containter name&gt; &lt;command in container&gt;
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CoreOS安装和使用Keepalived]]></title>
    <link href="http://duffqiu.github.io/blog/2015/07/05/coreos-keepalived/"/>
    <updated>2015-07-05T15:43:47+08:00</updated>
    <id>http://duffqiu.github.io/blog/2015/07/05/coreos-keepalived</id>
    <content type="html"><![CDATA[<h3 id="section">原由</h3>
<hr />
<p>在之前的文章<a href="/blog/2015/07/05/openstack-vip/">Openstack用Keealived来实现VIP</a>介绍了如何使用Keepalived来实现VIP，但是如果使用的vm是CoreOS，所有的程序又必须运行在Docker中的话，又该如何办呢？</p>

<h3 id="section-1">解决办法</h3>
<hr />

<ul>
  <li>将Keepalived做成一个Docker的image，如果需要可以自己下载keepalived的源码编译出可执行文件。因为只是用到VIP特性，可以在编译的时候将IPVS去掉（该功能可以使用haproxy代替）。一个比较偷懒的办法是先用yum的方式安装keepalived(基于CentOS)<code>yum install keepalived</code>，然后用<code>rpm -e --nodeps keepalived</code>去掉安装的可执行文件而是用自己编译的可执行文件。不然的话就需要一个一个指定的方式来安装keepalived需要的依赖包了</li>
  <li>查看CoreOS的内核是否已经启动ip_vs模块<code>lsmod</code>。似乎从681.2开始，默认都已经启动了，之前的版本没有默认启动</li>
  <li>如果没有启动ip_vs，则可以通过<code>sudo modprobe -a ip_vs</code>来加载</li>
  <li>使用<code>--priviliedged</code>和<code>--net=host</code>的方式来启动keepalived</li>
  <li>具体的docker file可以参考我的github上的<a href="https://github.com/duffqiu/keepalived">repo</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
